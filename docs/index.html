
<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
      <meta name="keywords" content="Robotics, Reinforcement Learning, Large Language Model">
      <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> Reward Design Framework Based on Reward Components and Large Language Models</title>
    <style>
        /* 设置水平线的粗细为 5px */
        hr {
            border: 0;
            border-top: 5px solid black;
        }
      
        .red-text {
            color: red;
        }


.blue-text {
    color: blue;
}
 .green-text {
    color: green;
}
        .yellow-text {
    color: yellow;
}
.purple-text {
    color: purple;
}
.orange-text {
    color: orange;
}
        .pink-text {
    color: pink;
}



.video-player {
  width: 200px;
  height: 200px;
  object-fit: cover;
}
/* @media (min-aspect-ratio: 16/9) {
  .video-player {
    width: 100vw;
    height: 56.25vw;
  }
} */

/* @media (max-aspect-ratio: 16/9) {
  .video-player {
    width: 768px;
    height: 500px;
  }
} */

    </style>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
    
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
     <link rel="icon" href="static/image/github-mark.png" type="image/x-icon">
    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
</head>
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Reward Design Framework Based on Reward Components and Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
             Kexin Jin<sup>1</sup>,</span>
            <span class="author-block">
             Guohui Tian<sup>2</sup>,</span>
            <span class="author-block">
              Bin Huang <sup>2</sup>,
            </span>
            <span class="author-block">
              Yongcheng Cui<sup>3</sup>,
            </span>
            <span class="author-block">
              Xiaoyu Zheng<sup>4</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
<!--             <span class="author-block"><sup>1</sup>Baidu RAL,</span> -->
            <span class="author-block"><sup>1</sup>Shandong University</span>
          </div>
            
         <div class="is-size-5 publication-authors">
                        <span class="author-block">
                                    <span class="icon">
                                        <i class="fa fa-envelope"></i>
                                    </span>
                            Corresponding authors: kx.jin@mail.sdu.edu.cn; g.h.tian@sdu.edu.cn;huangbin@sdu.edu.cn;cuiyc@mail.sdu.edu.cn;202334997@mail.sdu.edu.cn
                        </span>
                        </div>
          <div class="column has-text-centered">
            <div class="publication-links">


<!--               <span class="link-block">
                <a href="https://github.com/JKX-yy/ITN"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code </span>
                  </a>
              </span> -->

              <span class="link-block">
                <a href="static/Reward Design Framework Based on Reward Components and Large Language Models.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure style="display: flex; justify-content: center;">
         <a href="static/image/整体框架.png"><img src="static/image/整体框架.png" width="900px"></a>
      </figure>
      <h2 class="subtitle has-text-centered">
         Reward Design Framework Based on Reward Components and Large Language Models
      </h2>
    </div>
  </div>
</section>
    

 <!-- 显示加粗的线条 -->
    <hr>


   <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
<!--           Robots' working conditions and manipulation tasks are constantly changing in industrial environments. For robots to learn skills from scratch based on new task commands is a slow process that relies heavily on human assistance. Therefore, developing a mechanism that can adapt to different manipulation tasks and autonomously and quickly learn new skills can effectively address the issue of low efficiency in robot skill learning and enhance the robot's adaptive capabilities. This paper proposes a general Intelligent Transfer System (ITS) that enables robots to learn new skills rapidly and autonomously in dynamic tasks. ITS integrates Large Language Models (LLMs) with transfer reinforcement learning, leveraging LLMs' intelligence and prior skills knowledge. It can comprehend previously unseen task commands and automatically generate a process-oriented reward function based on task reward_components for each task, enabling the autonomous learning of new skills while eliminating the need to design hierarchical sub-processes for complex tasks. In addition, an Intelligent Transfer Network (ITN) is designed within ITS to extract knowledge of relevant skills and accelerate the learning of new ones. We systematically evaluate our method in the simulation environment. The results demonstrate that it can autonomously and efficiently learn unseen skills without relying on pre-programmed behavior, achieving true creativity while improving the time efficiency of two major tasks by 72.22% and 65.17% compared to learning from scratch.    -->
          The application of Large Language Models (LLMs) in the field of robotics has gained widespread attention. Due to their powerful code generation and contextual understanding capabilities, these models can generate reward functions from task commands, prompts, and environmental code, thus aiding robots in acquiring skills through reinforcement learning (RL). However, reward functions generated by existing models often suffer from issues of low executability and success rates, particularly when handling vague or complex task commands. These models usually require multiple iterations of optimization to achieve the desired outcomes. To address this challenge, we propose a reward function generation method based on reward components, which leverages human-level prior knowledge to improve generation efficiency and accuracy. Specifically, we have constructed a task→reward components dataset and fine-tuned a Reward Component Generator (RCG) using this dataset. The RCG then guides the automatic generation of reward functions for reinforcement learning tasks. Experimental results demonstrate that our method significantly improves reward executability, accuracy, and task success rate compared to state-of-the-art approaches.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>



<section class="section" style="flex: 1;">
    <div class="container is-max-desktop">
      <h3 class="title">Reward Components Dataset Construction</h3>
                <br>
            <span style="font-size: 125%">
<!--                  We have constructed a dataset containing 1000 instructions of task-reward_components. Each instruction includes task command information and the corresponding reward_components, as shown in the figure below. The dataset can be accessed in the <a href="https://github.com/JKX-yy/ITN">code</a>. -->
This dataset containing 10,000 industrial assembly commands, covering clear, ambiguous, complex, and specialized types of commands. The dataset includes both single-item and multi-item tasks, with over 200 industrial assembly components such as gears, washers, nuts, and nails.
                </span>
                     <br>

                        <br>
            <span style="font-size: 125%">
Each task command is annotated with computation components and conversion components based on the execution sequence. The following image shows the data saved in JSON format in our code.
                </span>
                     <br>
        
<!--数据集照片  -->
  <section class="hero teaser">
              <div class="container is-max-desktop">
                <div class="hero-body">
                  <figure style="display: flex; justify-content: center;">
                      <a href="static/image/dataset2.png"><img src="static/image/dataset2.png" width="900px"></a>
<!--                      <a href="static/image/datasets.png"><img src="/static/image/datasets.png" width="400px"></a> -->
                      
                  </figure>
                  <h2 class="subtitle has-text-centered">
                     Reward Components Dataset
                  </h2>
                </div>
              </div>
            </section>    

<!--四种指令示例  -->
                        <br>
            <span style="font-size: 125%">
The following will present four types of instructions in the dataset: clear, ambiguous, complex, and specialized commands.
                </span>
                     <br> 
<!--       第一种   清晰 -->
              <br>
      <h2>Examples of clear task commands</h2>
                     <br>
        
    <pre><code>
{
  "instruction": "List all the distance calculations involved and write all the necessary judgments based on the input task for the robot's continuous operation task.",
  "input": "The robotic arm first picks up the washer from the table and then places the washer on the bolt.",
  "output": "1. Calculate the distance between the fingertip_midpoint and the washer: distance_fingertip_midpoint_to_washer\n2. Determine whether the fingertip_midpoint are close enough to the washer: is_fingertip_midpoint_close_washer\n3. Calculate the distance between the washer and the bolt: distance_washer_to_bolt\n4. Determine whether the washer is aligned with the bolt: is_washer_aligned_bolt"
},
                </code></pre>

        <!--       第二种  模糊 -->
              <br>
      <h2>Examples of ambiguous task commands</h2>
                     <br>
        
    <pre><code>
{
  "instruction": "List all the distance calculations involved and write all the necessary judgments based on the input task for the robot's continuous operation task.",
  "input": "Insert the peg into the hole.",
  "output": "1. Calculate the distance between the fingertip_midpoint and the peg: distance_fingertip_midpoint_to_peg\n2. Determine whether the fingertip_midpoint are close enough to the peg: is_fingertip_midpoint_close_peg\n3. Calculate the distance between the peg and the hole: distance_peg_to_hole\n4. Determine whether the peg is aligned with the hole: is_peg_aligned_hole\n5. Calculate the distance between the peg and the bottow of the hole: distance_peg_to_bottowhole\n6. Determine whether the peg is aligned with the bottowhole: is_peg_aligned_bottowhole"
},
                </code></pre>

                <!--       第三种 复杂 -->
              <br>
      <h2>Examples of complex task commands</h2>
                     <br>
        
    <pre><code>
{
  "instruction": "For the continuous operation task of the robot involving 3 gears, list all the distance calculation components step by step.",
  "input": "The robot installs a large gear, a medium gear, and a small gear onto the rotating shaft.",
  "output": "1. Calculate the distance between the fingertip_midpoint and the large gear: distance_fingertip_midpoint_to_large_gear\n2. Calculate the distance between the large gear and the rotating shaft: distance_large_gear_to_shaft\n3. Calculate the distance between the fingertip_midpoint and the medium gear: distance_fingertip_midpoint_to_medium_gear\n4. Calculate the distance between the medium gear and the rotating shaft: distance_medium_gear_to_shaft\n5. Calculate the distance between the fingertip_midpoint and the small gear: distance_fingertip_midpoint_to_small_gear\n6. Calculate the distance between the small gear and the rotating shaft: distance_small_gear_to_shaft"
},
                </code></pre>
        

     <!--       第四种 专业 -->
              <br>
      <h2>Examples of specialized task commands</h2>
                     <br>
        
    <pre><code>
  {
    "instruction": "For the continuous operation task of the robot, according to the input task command, all the distance calculation components involved are listed step by step according to the execution order.",
    "input": "The robot mounts three rivets to a metal sheet, one at each corner except the top-right.",
    "output": "1. Calculate the distance between the fingertip_midpoint and the first rivet: distance_fingertip_midpoint_to_rivet_1\n2. Calculate the distance between the first rivet and the top-left corner of the sheet: distance_rivet_1_to_top_left\n3. Calculate the distance between the jaws and the second rivet: distance_fingertip_midpoint_to_rivet_2\n4. Calculate the distance between the second rivet and the bottom-left corner of the sheet: distance_rivet_2_to_bottom_left\n5. Calculate the distance between the jaws and the third rivet: distance_fingertip_midpoint_to_rivet_3\n6. Calculate the distance between the third rivet and the bottom-right corner of the sheet: distance_rivet_3_to_bottom_right"
  },
                </code></pre>

  
    </div>
  </section>
    
<!-- <!--微调LLAMA  -->


<section class="section" style="flex: 1;">
    <div class="container is-max-desktop">
      <h3 class="title">Reward Component Generator (RCG)</h3>
                <br>
        
            <span style="font-size: 125%">
               Based on the constructed Task→Reward Components dataset, this paper finetunes the open-source Large Language Model Meta AI (LLaMA) to learn the precise mapping between task commands and reward components, creating an efficient Reward Component Generator (RCG). 
                </span>
            <br>

                <br>
            <span style="font-size: 125%">
               Please refer to the fine-tuning process: 
                  </span>
            <br>

                  <span class="link-block">
                <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Instruction</span>
                  </a>
              </span>
                
                <br>
            <br>
            <span style="font-size: 125%">
               The following video shows a comparison video before and after fine-tuning the LLaMA on an NVIDIA RTX A5000 GPU using the constructed dataset.
              </span>
            <br>

<!--视频   微调前-->
                 <br>
                          <br>
                    <span style="font-size: 125%">
                        <span style="font-weight: bold; color: #FFA500;"> The following video demonstrates the output of the original LLaMA large model for the command :
          Push the three small, medium, and large-sized gears on the table in sequence to their designated positions.</span>
                    </span>
                    <br>
            <br>
                <div class="columns is-centered has-text-centered">
                  <div class="column is-two-thirds">

                    <video id="dollyzoom" autoplay controls muted allowfullscreen height="100%">
                      <source src="static/image/未微调.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>


            <section class="hero teaser">
              <div class="container is-max-desktop">
                <div class="hero-body">
                  <figure style="display: flex; justify-content: center;">
                     <a href="static/image/未微调.png"><img src="static/image/未微调.png" width="900px"></a>
                  </figure>
                  <h2 class="subtitle has-text-centered">
                     Not Fine-tuned
                  </h2>
                </div>
              </div>
            </section>

      <br>
      <h2>  <span style="font-weight: bold"> The output of the LLMs before fine-tuning is not directly usable for reward function generation, which is more oriented towards idea guidance.</span> </h2>
                     <br>


        
<!--微调后  -->
                   <br>

                            <span style="font-size: 125%">
                        <span style="font-weight: bold; color: #FFA500;"> The following video utilizes the RCG fine-tuned with the dataset and shows its results in generating the reward component :
          Push the three small, medium, and large-sized gears on the table in sequence to their designated positions.</span>
                    </span>
                    <br>
            <br>
                <div class="columns is-centered has-text-centered">
                  <div class="column is-two-thirds">

                    <video id="dollyzoom" autoplay controls muted allowfullscreen height="100%">
                      <source src="static/image/微调.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>


            <section class="hero teaser">
              <div class="container is-max-desktop">
                <div class="hero-body">
                  <figure style="display: flex; justify-content: center;">
                     <a href="static/image/微调.png"><img src="static/image/微调.png" width="900px"></a>
                  </figure>
                  <h2 class="subtitle has-text-centered">
                      Fine-tuned
                  </h2>
                </div>
              </div>
            </section>

      <br>
      <h2>  <span style="font-weight: bold"> The ability of RCG to accurately generate reward components reflects the accuracy of the reward component generation method in dealing with a variety of task instructions and provides data for further optimization of the reward function. This accuracy is critical for improving the method.</span> </h2>
                     <br>


        
        
    </div>
  </section>
    
    
<!-- 奖励函数生成实验 -->
<section class="section">
  <div class="container is-max-desktop">
    <h3 class="title">Reward Function Generation Experiment</h3>
      
      <p style="font-size: 125%">
In this experiment, we outlined the process of reward generation and optimization using the Reward Function Generator (RFG) based on reward components.
</p>
                <!--   生成器对简单技能的奖励函数生成和优化过程 -->
                  <br>
            <span style="font-size: 125%">
                <span style="font-weight: bold; color: #FFA500;"> The reward function generation and optimization process of the RFG. </span> 
            </span>
            <br>
             <br>
            <span style="font-size: 125%">
                 <span style="font-weight: bold"> Task description </span> : Push the medium-sized gears on the table to the target position
            </span>
            <br>


      
            <br>
            <span style="font-size: 125%">
                 <span style="font-weight: bold"> Environmental observation code </span> : The environment observation code includes the pose information of the Panda arm's gripper and the object's pose information.
            </span>
            <br>

                
    <pre><code>
class FactoryTaskNutBoltPick(FactoryEnvNutBolt, FactoryABCTask):
    """Rest of the environment definition omitted."""
    def compute_observations(self):
        """Compute observations."""
        obs_tensors = [self.fingertip_midpoint_pos,  
                       self.fingertip_midpoint_quat,  
                       self.fingertip_midpoint_linvel, 
                       self.fingertip_midpoint_angvel,
                        self.gear_medium_pos,  
                       self.gear_medium_quat,
                        self.base_medium_bolt_tip_pos,
                       self.base_medium_bolt_tip_quat,
                       ]
        self.obs_buf = torch.cat(obs_tensors, dim=-1)  
        pos=self.obs_buf.size()
        com_vector=torch.full([pos[0],self.num_observations-pos[1]],0.0).to(self.device)
        self.obs_buf = torch.cat((self.obs_buf,com_vector),-1)
        return self.obs_buf
                </code></pre>



                <!--  优化过程-->
                          <br>
                    <span style="font-size: 125%">
                         <span style="font-weight: bold"> The generation  process of the reward function.    </span>: 
                    </span>
                    <br>
<br>

<!--       原始LLMS  EUREKA -->

            <section class="hero teaser">
              <div class="container is-max-desktop">
                <div class="hero-body">
                  <figure style="display: flex; justify-content: center;">
                     <a href="static/image/eureka生成.png"><img src="static/image/eureka生成.png" width="900px"></a>
                  </figure>
                  <h2 class="subtitle has-text-centered">
                      Eureka
                  </h2>
                </div>
              </div>
            </section>

      <br>
       <span style="font-weight: bold"> The Eureka method mentioned above failed to analyze the hidden operator in the task instructions, resulting in a reward function that never included the gripper approaching the middle gear. This indicates that, under the guidance of this reward function, the task cannot be completed.   LLMs often struggle to automatically infer implicit dependency and sequence relationships (e.g., "manipulators → medium gear" and "medium gear → target position"). They also exhibit instability in identifying missing action subjects. This instability limits the reliability of LLMs in task planning and execution. </span> 
                     <br>
      
            <section class="hero teaser">
              <div class="container is-max-desktop">
                <div class="hero-body">
                  <figure style="display: flex; justify-content: center;">
                     <a href="static/image/本文.png"><img src="static/image/本文.png" width="900px"></a>
                  </figure>
                  <h2 class="subtitle has-text-centered">
                      RFG
                  </h2>
                </div>
              </div>
            </section>

      <br>
        <span style="font-weight: bold"> The method proposed in this paper leverages the accurately generated reward components from the RCG, demonstrating a high level of commands comprehension. It effectively handles ambiguous, complex, and specialized commands, rapidly generating reward functions capable of successfully completing tasks. </span> 
                     <br>

                                  <!--  优化过程-->
                          <br>
                    <span style="font-size: 125%">
                         <span style="font-weight: bold"> The optimization process of the reward function: RFG modifies and generates a new reward function through the computational analysis of the following process.    </span>: 
                    </span>
                    <br>

          <pre><code>
We trained a RL policy using the provided reward function code and tracked the values of the individual components in the reward function as well as global policy metrics such as success rates and episode lengths after every 11 epochs and the maximum, mean, minimum values encountered:
reward_pick: ['0.05', '0.06', '0.17', '0.40', '0.68', '0.72', '0.77', '0.79', '0.78', '0.79', '0.79'], Max: 0.80, Mean: 0.57, Min: 0.05 
reward_lift: ['0.83', '0.83', '0.83', '0.83', '0.83', '0.83', '0.83', '0.84', '0.84', '0.86', '0.85'], Max: 0.86, Mean: 0.84, Min: 0.83 
task_score: ['0.00', '0.00', '0.00', '0.02', '0.30', '0.53', '0.67', '0.77', '0.80', '0.87', '0.77'], Max: 0.91, Mean: 0.46, Min: 0.00 
episode_lengths: ['119.00', '119.00', '119.00', '119.00', '119.00', '119.00', '119.00', '119.00', '119.00', '119.00', '119.00'], Max: 119.00, Mean: 119.00, Min: 119.00 
Please carefully analyze the policy feedback and provide a new, improved reward function that can better solve the task. Some helpful tips for analyzing the policy feedback:
    (1) If the success rate is always close to zero, it means that the sequential execution has failed to reach the last step, and you can infer that the task execution failed at the first step by analyzing the rewards for each subtask
    (2) If the values for a certain reward_components are near identical throughout, then this means RL is not able to optimize this component as it is written. You may consider
        (a) Changing its scale or the value of its temperature parameter
        (b) Re-writing the reward_components 
        (c) Discarding the reward_components
    (3) If some reward_components' magnitude is significantly larger, then you must re-scale its value to a proper range
Please analyze each existing reward_components in the suggested manner above first, and then write the reward function code. The output of the reward function should consist of two items:
    (1) the total reward,
    (2) a dictionary of each individual reward_components(reward dictionary for each sub-process)..
The code output should be formatted as a python code string: "```python ... ```".
                </code></pre>





      

  
    <!--       3.注意力 -->

           <br>
                      <span style="font-size: 125%">
                        <span style="font-weight: bold; color: #FFA500;">  CONCLUSIONS</span>
      This paper proposes a reward design framework based on r efficiency of reward generation. We constructed a dataset of 10,000 task→reward components for common industrial assembly tasks and fine-tuned the reward component generator to handle complex, ambiguous commands and enable precise, logical planning, thereby providing clear guidance for reward generation. Experimental results validate the superiority of this method, offering a more efficient solution for robot skill learning 
                    </span>
                    <br>
<!--               <br>
       <figure style="display: flex; justify-content: center;">
         <a href="static/attention.png"><img src="static/attention.png" width="900px"></a>
      </figure>
                       <br>
                      <span style="font-size: 125%">
                         The figure shows ITN's policy allocation concerns. The data, derived from a single interaction cycle after training, stabilized and revealed that Intelligent Attention allocated attention to source skills differently at various stages of the target task. This distribution is similar to human experience. ITN successfully adapted to various tasks by adjusting the attention values of source skills, enabling robots to master new tasks quickly. 
                      </span>
                    <br> -->

      </div>
</section>



    
<!-- <section class="section">
  <div class="container is-max-desktop">
      <p style="font-size: 125%">
  We compare ITN(ours) with two state-of-the-art  learning methods FSAT,H-DRLN,The following figure shows the performance of these three methods in two tasks.
</p>
      <br>
<h3 class="title is-4"><span class="dvima"style="color: red;">Comparison With Other Advanced Methods</span></h3>
    <img src="static/image/compare.png" class="static/image/compare.png" alt=""
        style="display: block; margin-left: auto; margin-right: auto" />
    <br>
    <span style="font-size: 125%">
        <span style="font-weight: bold"> ITN is superior to other methods. </span> Several methods of learning significantly outperformed direct learning. Although FSAT performed best in early training by selecting data closer to the target domain to initialize the policy, it was surpassed by ITN after a few dozen iterations. FSAT focused on extracting shared features and excelled in solving transfer problems across manipulated objects, whereas ITN focused on exploring a generalized transfer approach across tasks. H-DRLN attempted to receive the current environment state as input and output the probabilities of new actions and source skills, reusing the subskill with the highest probability value. Although this approach incorporated learning new strategies, the reuse approach was very limited in terms of usage scenarios. ITN was not dependent on specific skills and could fully adapt to task changes.
    </span>
    <br>

       <img src="static/表1.png" class="static/表1.png" alt=""
        style="display: block; margin-left: auto; margin-right: auto" />
    <br>
    <span style="font-size: 125%">
        <span style="font-weight: bold"> Utilizing the ITN method to learn skills can result in a high success rate in a short period of time. </span> Utilizing the ITN method to learn skills can result in a high success rate in a short period. Our (ITN) method excelled in the early and middle stages, achieving an initial success rate of 86.64% and 79.56% in two tasks, significantly higher than the other methods. As training progressed, the success rate of all methods rose, but ITN achieved a higher success rate at an earlier stage, demonstrating superior time efficiency. The method is highly anticipated for its practical value.
        <br>
        <br>
        <br>
    <h4 class="title" >Other Experiments</h4>
        <br>
      <h3 class="title is-4"><span class="dvima"style="color: green;">Verification Experiment</span></h3>
    <img src="static/abla.png" class="static/abla.png" alt=""
        style="display: block; margin-left: auto; margin-right: auto" />
    <br>
    <span style="font-size: 125%">
          <span style="font-weight: bold">ITN had the ability to quickly master new tasks.</span>The learning speed (the epoch at which the curve reached stability) of ITN was significantly better than No\_Transfer in both tasks.PST simulates the absence of prior skills. The results demonstrated that ITN learns much faster than No\_Transfer with just a few prior skills, indicating that ITN can rapidly acquire new skills that are not based on existing ones. This capability is a significant goal and breakthrough of our work, as most previous studies have relied on fixed skill combinations. Comparing the performance of ITN with NIA demonstrated the superior effectiveness of integrating an Intelligent Attention model over a fixed value. The performance comparison between NIA and DFS confirmed the strong feature fusion capability of the feature fusion layer. ITN improved time efficiency by approximately 72.22\% and 65.17\% compared to No\_Transfer in the respective tasks.
           </span>
        <br>
  <br>
      <h3 class="title is-4"><span class="dvima"style="color: purple;">Generalization test experiment</span></h3>
    <img src="static/ge.png" class="static/ge.png" alt=""
        style="display: block; margin-left: auto; margin-right: auto" />
    <br>
    <span style="font-size: 125%">
      <span style="font-weight: bold"> ITN had the ability to transfer across tasks.</span> Curves 2 and 3 revealed that, although the target task and the source skills had different manipulation goals, ITN effectively utilized shared action features for transfer. Based on the action feature level, this transfer effectively addressed various manipulation problems encountered by robots in dynamic tasks. Curve 4 indicated that the robot quickly achieved high rewards when operating unseen new parts, highlighting ITN's efficiency in handling task variations. Curve 5 showed the best learning speed and the most stable 
learning curve throughout the training process compared to the other curves, suggesting that the multiple-skill fusion strategy was the most effective, significantly enhancing the learning efficiency and performance of the robot manipulation tasks.

    <br>
      
  </div>
</section> -->

    
 <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a
                                href="https://eureka-research.github.io/">Eureka</a> and
                            <a href="https://rlingua.github.io/">Rlingua</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>
